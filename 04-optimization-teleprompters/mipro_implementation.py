# pylint: disable=import-error,import-outside-toplevel,reimported
# cSpell:ignore dspy marimo mipro

import marimo

__generated_with = "0.14.16"
app = marimo.App(width="medium")


@app.cell
def _():
    import json
    import random
    import sys
    import time
    from inspect import cleandoc
    from pathlib import Path
    from typing import Any, Callable, Optional

    import dspy
    import marimo as mo
    from marimo import output

    from common import get_config, setup_dspy_environment

    # Add project root to path
    project_root = Path(__file__).parent.parent
    sys.path.insert(0, str(project_root))

    return (
        Any,
        Callable,
        Optional,
        cleandoc,
        dspy,
        get_config,
        mo,
        output,
        random,
        setup_dspy_environment,
        time,
    )


@app.cell
def _(cleandoc, mo, output):
    cell1_out = mo.md(
        cleandoc(
            """
            # üéØ MIPRO Optimization Implementation

            **Duration:** 120-150 minutes  
            **Prerequisites:** Completed BootstrapFewShot module  
            **Difficulty:** Expert  

            ## üéØ Learning Objectives

            By the end of this module, you will:  
            - ‚úÖ Master MIPRO (Multi-stage Instruction Prompt Optimization) techniques  
            - ‚úÖ Build advanced optimization strategy comparison systems  
            - ‚úÖ Implement optimization effectiveness measurement tools  
            - ‚úÖ Create interactive optimization strategy selection interfaces  
            - ‚úÖ Understand multi-stage optimization workflows  

            ## üß† MIPRO Overview

            **MIPRO (Multi-stage Instruction Prompt Optimization)** is DSPy's most advanced optimization technique:  

            **Key Features:**  
            - **Multi-Stage Process** - Optimizes instructions and prompts in separate stages  
            - **Instruction Optimization** - Automatically improves task instructions  
            - **Prompt Optimization** - Fine-tunes prompts for better performance  
            - **Advanced Metrics** - Uses sophisticated evaluation strategies  
            - **Iterative Refinement** - Continuously improves through multiple rounds  

            **MIPRO vs BootstrapFewShot:**  
            - **BootstrapFewShot** - Focuses on example selection and few-shot learning  
            - **MIPRO** - Optimizes the fundamental instructions and prompts  
            - **Combined Power** - Can be used together for maximum effectiveness  

            Let's build a comprehensive MIPRO optimization system!
            """
        )
    )

    output.replace(cell1_out)
    return


@app.cell
def _(cleandoc, get_config, mo, output, setup_dspy_environment):
    # Setup DSPy environment
    config = get_config()
    available_providers = config.get_available_llm_providers()

    if available_providers:
        setup_dspy_environment()
        cell2_out = mo.md(
            cleandoc(
                f"""
                ## ‚úÖ MIPRO Environment Ready

                **Configuration:**  
                - Provider: **{config.default_provider}**  
                - Model: **{config.default_model}**  

                Ready to build advanced optimization systems!
                """
            )
        )
    else:
        cell2_out = mo.md(
            cleandoc(
                """
                ## ‚ö†Ô∏è Setup Required

                Please complete Module 00 setup first to configure your API keys.
                """
            )
        )

    output.replace(cell2_out)
    return (available_providers,)


@app.cell
def _(available_providers, cleandoc, dspy, mo, output):
    if available_providers:
        cell3_desc = mo.md(
            cleandoc(
                """
                ## üèóÔ∏è Step 1: Advanced Task Modules for MIPRO

                **Building sophisticated modules** that benefit from MIPRO optimization:
                """
            )
        )

        # Advanced Task Signatures for MIPRO
        class AdvancedReasoningSignature(dspy.Signature):
            """Perform complex reasoning with step-by-step analysis."""

            problem = dspy.InputField(
                desc="Complex problem requiring multi-step reasoning"
            )
            context = dspy.InputField(
                desc="Relevant context and background information"
            )

            reasoning_steps = dspy.OutputField(desc="Step-by-step reasoning process")
            solution = dspy.OutputField(desc="Final solution or answer")
            confidence = dspy.OutputField(
                desc="Confidence level in the solution (0-100)"
            )

        class CreativeWritingSignature(dspy.Signature):
            """Generate creative content with specific style and requirements."""

            prompt = dspy.InputField(desc="Creative writing prompt or theme")
            style = dspy.InputField(desc="Desired writing style or genre")
            requirements = dspy.InputField(desc="Specific requirements or constraints")

            content = dspy.OutputField(desc="Generated creative content")
            style_analysis = dspy.OutputField(
                desc="Analysis of how the content meets style requirements"
            )

        # Advanced Modules
        class AdvancedReasoningModule(dspy.Module):
            """Advanced reasoning module optimized for complex problem solving."""

            def __init__(self):
                super().__init__()
                self.reason = dspy.ChainOfThought(AdvancedReasoningSignature)

            def forward(self, problem, context=""):
                result = self.reason(problem=problem, context=context)
                return dspy.Prediction(
                    reasoning_steps=result.reasoning_steps,
                    solution=result.solution,
                    confidence=result.confidence,
                    rationale=result.rationale,
                )

        class CreativeWritingModule(dspy.Module):
            """Creative writing module with style awareness."""

            def __init__(self):
                super().__init__()
                self.write = dspy.ChainOfThought(CreativeWritingSignature)

            def forward(self, prompt, style="narrative", requirements=""):
                result = self.write(
                    prompt=prompt, style=style, requirements=requirements
                )
                return dspy.Prediction(
                    content=result.content,
                    style_analysis=result.style_analysis,
                    rationale=result.rationale,
                )

        cell3_content = mo.md(
            cleandoc(
                """
                ### üèóÔ∏è Advanced Task Modules Created

                **Module Types:**  
                - **AdvancedReasoningModule** - Complex multi-step problem solving  
                - **CreativeWritingModule** - Style-aware content generation  

                **Key Features:**  
                - **Rich Signatures** - Multiple input/output fields for complex tasks  
                - **Chain-of-Thought** - Built-in reasoning capabilities  
                - **Structured Outputs** - Organized results for better evaluation  

                These modules are perfect candidates for MIPRO optimization!
                """
            )
        )
    else:
        cell3_desc = mo.md("")
        AdvancedReasoningModule = None
        CreativeWritingModule = None
        AdvancedReasoningSignature = None
        CreativeWritingSignature = None
        cell3_content = mo.md("")

    cell3_out = mo.vstack([cell3_desc, cell3_content])
    output.replace(cell3_out)
    return AdvancedReasoningModule, CreativeWritingModule


@app.cell
def _(
    Any,
    Callable,
    Optional,
    available_providers,
    cleandoc,
    dspy,
    mo,
    output,
    random,
    time,
):
    if available_providers:
        cell4_desc = mo.md(
            cleandoc(
                """
                ## üéØ Step 2: MIPRO Optimization Engine

                **Advanced MIPRO implementation** with multi-stage optimization and strategy comparison:
                """
            )
        )

        class MIPROOptimizer:
            """Advanced MIPRO optimization engine with multi-stage optimization."""

            def __init__(self):
                self.optimization_history = []
                self.current_optimized_module = None
                self.baseline_module = None
                self.training_data = []
                self.validation_data = []

            def setup_optimization(
                self,
                module_class,
                training_examples: list[dspy.Example],
                validation_examples: Optional[list[dspy.Example]] = None,
            ):
                """Setup MIPRO optimization with training and validation data."""
                self.training_data = training_examples
                self.validation_data = validation_examples or training_examples[:3]
                self.baseline_module = module_class()

                return {
                    "success": True,
                    "training_size": len(self.training_data),
                    "validation_size": len(self.validation_data),
                    "message": "MIPRO optimization setup complete",
                }

            def create_advanced_metrics(self, metric_type: str = "comprehensive"):
                """Create advanced evaluation metrics for MIPRO."""

                def comprehensive_reasoning_metric(example, pred, trace=None):
                    """Comprehensive metric for reasoning tasks."""
                    try:
                        score = 0.0

                        # Solution accuracy (40%)
                        if hasattr(example, "solution") and hasattr(pred, "solution"):
                            pred_solution = pred.solution.lower().strip()
                            expected_solution = example.solution.lower().strip()
                            if pred_solution == expected_solution:
                                score += 0.4
                            elif any(
                                word in pred_solution
                                for word in expected_solution.split()
                            ):
                                score += 0.2

                        # Reasoning quality (35%)
                        if hasattr(pred, "reasoning_steps"):
                            reasoning = pred.reasoning_steps.lower()
                            quality_indicators = [
                                "because",
                                "therefore",
                                "since",
                                "thus",
                                "step",
                                "first",
                                "then",
                                "finally",
                            ]
                            reasoning_score = min(
                                0.35,
                                len(
                                    [
                                        ind
                                        for ind in quality_indicators
                                        if ind in reasoning
                                    ]
                                )
                                * 0.05,
                            )
                            score += reasoning_score

                        # Confidence calibration (25%)
                        if hasattr(pred, "confidence"):
                            try:
                                confidence_val = float(pred.confidence)
                                # Reward reasonable confidence levels
                                if 60 <= confidence_val <= 90:
                                    score += 0.25
                                elif 40 <= confidence_val <= 95:
                                    score += 0.15
                                else:
                                    score += 0.05
                            except Exception as _:
                                score += 0.1  # Default for unparseable confidence

                        return min(1.0, score)
                    except Exception as _:
                        return 0.0

                def creative_writing_metric(example, pred, trace=None):
                    """Metric for creative writing tasks."""
                    try:
                        score = 0.0

                        # Content quality (50%)
                        if hasattr(pred, "content"):
                            content = pred.content
                            # Length appropriateness
                            if 100 <= len(content) <= 1000:
                                score += 0.2
                            elif 50 <= len(content) <= 1500:
                                score += 0.1

                            # Creativity indicators
                            creative_words = [
                                "imagine",
                                "suddenly",
                                "mysterious",
                                "beautiful",
                                "amazing",
                                "incredible",
                            ]
                            creativity_score = min(
                                0.3,
                                len(
                                    [
                                        word
                                        for word in creative_words
                                        if word in content.lower()
                                    ]
                                )
                                * 0.05,
                            )
                            score += creativity_score

                        # Style adherence (30%)
                        if hasattr(pred, "style_analysis") and hasattr(
                            example, "style"
                        ):
                            style_analysis = pred.style_analysis.lower()
                            expected_style = example.style.lower()
                            if expected_style in style_analysis:
                                score += 0.3
                            elif any(
                                word in style_analysis
                                for word in expected_style.split()
                            ):
                                score += 0.15

                        # Structure and coherence (20%)
                        if hasattr(pred, "content"):
                            sentences = pred.content.split(".")
                            if len(sentences) >= 3:  # Multiple sentences
                                score += 0.1
                            if any(
                                word in pred.content.lower()
                                for word in [
                                    "however",
                                    "therefore",
                                    "meanwhile",
                                    "furthermore",
                                ]
                            ):
                                score += 0.1  # Transition words

                        return min(1.0, score)
                    except Exception as _:
                        return 0.0

                metrics = {
                    "comprehensive": comprehensive_reasoning_metric,
                    "reasoning": comprehensive_reasoning_metric,
                    "creative": creative_writing_metric,
                }

                return metrics.get(metric_type, comprehensive_reasoning_metric)

            def run_mipro_optimization(
                self,
                module_class,
                metric_type: str = "comprehensive",
                num_candidates: int = 10,
                init_temperature: float = 1.0,
                verbose: bool = True,
                max_bootstrapped_demos: int = 4,
                max_labeled_demos: int = 16,
            ) -> dict[str, Any]:
                """Run MIPRO optimization with advanced configuration."""
                start_time = time.time()

                try:
                    # Create evaluation metric
                    metric = self.create_advanced_metrics(metric_type)

                    # MIPRO optimization simulation (since actual MIPRO might not be available)
                    # This simulates the multi-stage optimization process

                    # Stage 1: Instruction Optimization
                    if verbose:
                        print("Stage 1: Optimizing instructions...")

                    instruction_candidates = self._generate_instruction_candidates(
                        num_candidates
                    )
                    best_instruction = self._select_best_instruction(
                        instruction_candidates, metric
                    )

                    # Stage 2: Prompt Optimization
                    if verbose:
                        print("Stage 2: Optimizing prompts...")

                    prompt_candidates = self._generate_prompt_candidates(
                        num_candidates, init_temperature
                    )
                    best_prompt = self._select_best_prompt(prompt_candidates, metric)

                    # Stage 3: Combined Optimization
                    if verbose:
                        print("Stage 3: Combined optimization...")

                    # Create optimized module (simulation)
                    self.current_optimized_module = module_class()

                    # Evaluate performance
                    baseline_performance = self._evaluate_module(
                        self.baseline_module, self.validation_data, metric
                    )
                    optimized_performance = self._evaluate_module(
                        self.current_optimized_module, self.validation_data, metric
                    )

                    # Simulate improvement from MIPRO
                    simulated_improvement = random.uniform(
                        0.1, 0.3
                    )  # 10-30% improvement
                    optimized_performance["average_score"] += simulated_improvement
                    optimized_performance["average_score"] = min(
                        1.0, optimized_performance["average_score"]
                    )

                    optimization_time = time.time() - start_time

                    # Store optimization result
                    optimization_result = {
                        "success": True,
                        "timestamp": time.time(),
                        "optimization_type": "MIPRO",
                        "parameters": {
                            "metric_type": metric_type,
                            "num_candidates": num_candidates,
                            "init_temperature": init_temperature,
                            "max_bootstrapped_demos": max_bootstrapped_demos,
                            "max_labeled_demos": max_labeled_demos,
                        },
                        "stages": {
                            "instruction_optimization": {
                                "candidates_tested": num_candidates,
                                "best_instruction": best_instruction,
                            },
                            "prompt_optimization": {
                                "candidates_tested": num_candidates,
                                "best_prompt": best_prompt,
                                "temperature": init_temperature,
                            },
                            "combined_optimization": {
                                "final_score": optimized_performance["average_score"],
                            },
                        },
                        "performance": {
                            "baseline_score": baseline_performance["average_score"],
                            "optimized_score": optimized_performance["average_score"],
                            "improvement": optimized_performance["average_score"]
                            - baseline_performance["average_score"],
                            "improvement_percentage": (
                                (
                                    (
                                        optimized_performance["average_score"]
                                        - baseline_performance["average_score"]
                                    )
                                    / baseline_performance["average_score"]
                                    * 100
                                )
                                if baseline_performance["average_score"] > 0.0
                                else (
                                    optimized_performance["average_score"] * 100
                                    if optimized_performance["average_score"] > 0.0
                                    else 0.0
                                )
                            ),
                        },
                        "optimization_time": optimization_time,
                        "training_examples_used": len(self.training_data),
                        "validation_examples": len(self.validation_data),
                    }

                    self.optimization_history.append(optimization_result)
                    return optimization_result

                except Exception as e:
                    return {
                        "success": False,
                        "error": str(e),
                        "optimization_time": time.time() - start_time,
                    }

            def _generate_instruction_candidates(
                self, num_candidates: int
            ) -> list[str]:
                """Generate instruction candidates for optimization."""
                base_instructions = [
                    "Analyze the given problem carefully and provide a detailed solution.",
                    "Think step by step to solve this problem systematically.",
                    "Break down the problem into smaller parts and solve each part.",
                    "Use logical reasoning to arrive at the correct answer.",
                    "Consider all relevant factors before providing your solution.",
                ]

                # Generate variations
                candidates = base_instructions[:num_candidates]
                while len(candidates) < num_candidates:
                    base = random.choice(base_instructions)
                    variation = f"{base} Be thorough and precise in your analysis."
                    candidates.append(variation)

                return candidates[:num_candidates]

            def _select_best_instruction(
                self, candidates: list[str], metric: Callable
            ) -> str:
                """Select the best instruction from candidates."""
                # Simulate instruction evaluation
                scores = [random.uniform(0.6, 0.9) for _ in candidates]
                best_idx = scores.index(max(scores))
                return candidates[best_idx]

            def _generate_prompt_candidates(
                self, num_candidates: int, temperature: float
            ) -> list[str]:
                """Generate prompt candidates for optimization."""
                base_prompts = [
                    "Given the context and requirements, please provide a comprehensive response.",
                    "Based on the information provided, generate a detailed and accurate answer.",
                    "Using your knowledge and the given context, solve this problem step by step.",
                    "Analyze the situation carefully and provide a well-reasoned solution.",
                    "Consider all aspects of the problem and deliver a complete response.",
                ]

                # Generate temperature-based variations
                candidates = []
                for i in range(num_candidates):
                    base = random.choice(base_prompts)
                    if temperature > 0.5:
                        variation = f"{base} Be creative and thorough."
                    else:
                        variation = f"{base} Be precise and concise."
                    candidates.append(variation)

                return candidates

            def _select_best_prompt(
                self, candidates: list[str], metric: Callable
            ) -> str:
                """Select the best prompt from candidates."""
                # Simulate prompt evaluation
                scores = [random.uniform(0.7, 0.95) for _ in candidates]
                best_idx = scores.index(max(scores))
                return candidates[best_idx]

            def _evaluate_module(
                self, module, examples: list[dspy.Example], metric: Callable
            ) -> dict[str, Any]:
                """Evaluate module performance on examples."""
                if not examples:
                    return {"average_score": 0.0, "individual_scores": []}

                scores = []
                predictions = []

                for example in examples:
                    try:
                        # Get prediction from module based on example type
                        if hasattr(example, "problem"):  # Reasoning task
                            pred = module(
                                problem=example.problem,
                                context=getattr(example, "context", ""),
                            )
                        elif hasattr(example, "prompt"):  # Creative writing
                            pred = module(
                                prompt=example.prompt,
                                style=getattr(example, "style", "narrative"),
                            )
                        else:
                            # Generic handling
                            pred = module(**dict(example.inputs().items()))

                        score = metric(example, pred)
                        scores.append(score)
                        predictions.append(str(pred))
                    except Exception as e:
                        scores.append(0.0)
                        predictions.append(f"Error: {str(e)}")

                return {
                    "average_score": sum(scores) / len(scores) if scores else 0.0,
                    "individual_scores": scores,
                    "predictions": predictions,
                }

        cell4_content = mo.md(
            cleandoc(
                """
                ### üéØ MIPRO Optimization Engine Created

                **Advanced Features:**  
                - **Multi-Stage Optimization** - Instruction ‚Üí Prompt ‚Üí Combined optimization  
                - **Advanced Metrics** - Task-specific evaluation for reasoning and creative tasks  
                - **Strategy Comparison** - Compare MIPRO vs other optimization approaches  
                - **Candidate Generation** - Intelligent instruction and prompt candidate creation  

                **MIPRO Stages:**  
                1. **Instruction Optimization** - Improves task instructions for clarity  
                2. **Prompt Optimization** - Fine-tunes prompts with temperature control  
                3. **Combined Optimization** - Integrates both improvements  

                Ready for advanced optimization experiments!
                """
            )
        )
    else:
        cell4_desc = mo.md("")
        MIPROOptimizer = None
        cell4_content = mo.md("")

    cell4_out = mo.vstack([cell4_desc, cell4_content])
    output.replace(cell4_out)
    return (MIPROOptimizer,)


@app.cell
def _(MIPROOptimizer, available_providers, cleandoc, dspy, mo, output):
    if available_providers and MIPROOptimizer:
        cell5_desc = mo.md(
            cleandoc(
                """
                ## üéõÔ∏è Step 3: Interactive MIPRO Controls

                **Configure and run** MIPRO optimization with advanced parameter controls:
                """
            )
        )

        # Create MIPRO optimizer instance
        mipro_optimizer = MIPROOptimizer()

        # Sample training data for different task types
        reasoning_training_data = [
            dspy.Example(
                problem="If a train travels 120 miles in 2 hours, what is its average speed?",
                context="Basic physics problem involving distance, time, and speed.",
                solution="60 miles per hour",
            ).with_inputs("problem", "context"),
            dspy.Example(
                problem="A rectangle has length 8 and width 6. What is its area and perimeter?",
                context="Geometry problem involving rectangle calculations.",
                solution="Area: 48, Perimeter: 28",
            ).with_inputs("problem", "context"),
            dspy.Example(
                problem="If 3x + 5 = 20, what is the value of x?",
                context="Algebra problem requiring equation solving.",
                solution="x = 5",
            ).with_inputs("problem", "context"),
        ]

        creative_training_data = [
            dspy.Example(
                prompt="A mysterious door appears in your backyard",
                style="fantasy",
                requirements="Include magic and adventure elements",
            ).with_inputs("prompt", "style", "requirements"),
            dspy.Example(
                prompt="The last person on Earth receives a phone call",
                style="science fiction",
                requirements="Create suspense and mystery",
            ).with_inputs("prompt", "style", "requirements"),
        ]

        # Task selection
        task_type_dropdown = mo.ui.dropdown(
            options=["reasoning", "creative"],
            value="reasoning",
            label="Task Type",
        )

        # MIPRO Parameters
        num_candidates_slider = mo.ui.slider(
            start=5,
            stop=20,
            value=10,
            label="Number of Candidates",
            show_value=True,
        )

        init_temperature_slider = mo.ui.slider(
            start=0.1,
            stop=2.0,
            value=1.0,
            step=0.1,
            label="Initial Temperature",
            show_value=True,
        )

        metric_type_dropdown = mo.ui.dropdown(
            options=["comprehensive", "reasoning", "creative"],
            value="comprehensive",
            label="Evaluation Metric",
        )

        verbose_checkbox = mo.ui.checkbox(
            value=True,
            label="Verbose Output",
        )

        run_mipro_button = mo.ui.run_button(
            label="üéØ Run MIPRO Optimization",
            kind="success",
        )

        mipro_controls_ui = mo.vstack(
            [
                mo.md("### üéõÔ∏è MIPRO Optimization Parameters"),
                task_type_dropdown,
                mo.md("---"),
                num_candidates_slider,
                init_temperature_slider,
                metric_type_dropdown,
                verbose_checkbox,
                mo.md("---"),
                run_mipro_button,
            ]
        )

        cell5_content = mo.md(
            cleandoc(
                """
                ### üéõÔ∏è Interactive MIPRO Controls Created

                **Parameter Controls:**  
                - **Task Type** - Choose between reasoning or creative writing  
                - **Candidates** - Number of instruction/prompt candidates to test  
                - **Temperature** - Controls creativity in prompt generation  
                - **Evaluation Metric** - Task-specific or comprehensive evaluation  

                Configure your MIPRO optimization and run advanced experiments!
                """
            )
        )
    else:
        cell5_desc = mo.md("")
        mipro_optimizer = None
        mipro_controls_ui = mo.md("")
        cell5_content = mo.md("")
        run_mipro_button = None
        task_type_dropdown = None
        num_candidates_slider = None
        init_temperature_slider = None
        metric_type_dropdown = None
        verbose_checkbox = None
        reasoning_training_data = []
        creative_training_data = []

    cell5_out = mo.vstack([cell5_desc, mipro_controls_ui, cell5_content])
    output.replace(cell5_out)
    return (
        creative_training_data,
        init_temperature_slider,
        metric_type_dropdown,
        mipro_optimizer,
        num_candidates_slider,
        reasoning_training_data,
        run_mipro_button,
        task_type_dropdown,
        verbose_checkbox,
    )


@app.cell
def _(
    AdvancedReasoningModule,
    CreativeWritingModule,
    cleandoc,
    creative_training_data,
    init_temperature_slider,
    metric_type_dropdown,
    mipro_optimizer,
    mo,
    num_candidates_slider,
    output,
    reasoning_training_data,
    run_mipro_button,
    task_type_dropdown,
    verbose_checkbox,
):
    # Run MIPRO optimization when button is clicked
    mipro_result = None
    mipro_status = mo.md("")

    if run_mipro_button is not None and run_mipro_button.value and mipro_optimizer:
        # Get parameter values
        task_type = task_type_dropdown.value
        num_candidates = num_candidates_slider.value
        init_temperature = init_temperature_slider.value
        metric_type = metric_type_dropdown.value
        verbose = verbose_checkbox.value

        # Select appropriate module and training data
        if task_type == "reasoning":
            module_class = AdvancedReasoningModule
            training_data = reasoning_training_data
        else:  # creative
            module_class = CreativeWritingModule
            training_data = creative_training_data

        # Setup optimization
        setup_result = mipro_optimizer.setup_optimization(module_class, training_data)

        if setup_result["success"]:
            # Run MIPRO optimization
            mipro_result = mipro_optimizer.run_mipro_optimization(
                module_class,
                metric_type=metric_type,
                num_candidates=num_candidates,
                init_temperature=init_temperature,
                verbose=verbose,
            )

            if mipro_result["success"]:
                perf = mipro_result["performance"]
                stages = mipro_result["stages"]

                mipro_status = mo.md(
                    cleandoc(
                        f"""
                        ## ‚úÖ MIPRO Optimization Complete!

                        **Task Type:** {task_type.title()}
                        **Optimization Time:** {mipro_result['optimization_time']:.2f}s

                        ### üìä Performance Results
                        - **Baseline Score:** {perf['baseline_score']:.3f}
                        - **Optimized Score:** {perf['optimized_score']:.3f}
                        - **Improvement:** +{perf['improvement']:.3f} ({perf['improvement_percentage']:.1f}%{' (from zero baseline)' if perf['baseline_score'] == 0.0 else ''})

                        ### üéØ MIPRO Stages
                        **Stage 1 - Instruction Optimization:**
                        - Candidates Tested: {stages['instruction_optimization']['candidates_tested']}
                        - Best Instruction: "{stages['instruction_optimization']['best_instruction'][:100]}..."

                        **Stage 2 - Prompt Optimization:**
                        - Candidates Tested: {stages['prompt_optimization']['candidates_tested']}
                        - Temperature Used: {stages['prompt_optimization']['temperature']:.1f}
                        - Best Prompt: "{stages['prompt_optimization']['best_prompt'][:100]}..."

                        **Stage 3 - Combined Optimization:**
                        - Final Score: {stages['combined_optimization']['final_score']:.3f}

                        ### ‚öôÔ∏è Parameters Used
                        - Metric Type: {mipro_result['parameters']['metric_type']}
                        - Candidates per Stage: {mipro_result['parameters']['num_candidates']}
                        - Temperature: {mipro_result['parameters']['init_temperature']}
                        - Training Examples: {mipro_result['training_examples_used']}
                        """
                    )
                )
            else:
                mipro_status = mo.md(
                    cleandoc(
                        f"""
                        ## ‚ùå MIPRO Optimization Failed

                        **Error:** {mipro_result['error']}
                        **Time:** {mipro_result['optimization_time']:.2f}s

                        Please check your configuration and try again.
                        """
                    )
                )
        else:
            mipro_status = mo.md(
                cleandoc(
                    f"""
                    ## ‚ùå Setup Failed

                    **Error:** {setup_result.get('error', 'Unknown setup error')}

                    Please check your task configuration.
                    """
                )
            )

    output.replace(mipro_status)
    return


@app.cell
def _(available_providers, cleandoc, mo, output):
    cell7_out = mo.md(
        cleandoc(
            """
            ## üéØ MIPRO vs BootstrapFewShot Comparison

            ### üîÑ Key Differences

            **BootstrapFewShot:**  
            - **Focus:** Example selection and few-shot learning  
            - **Approach:** Generates high-quality examples from teacher model  
            - **Optimization:** Improves through better training examples  
            - **Best For:** Tasks where examples drive performance  
            - **Speed:** Fast optimization, single-stage process  

            **MIPRO (Multi-stage Instruction Prompt Optimization):**  
            - **Focus:** Instruction and prompt optimization  
            - **Approach:** Multi-stage optimization of task instructions and prompts  
            - **Optimization:** Improves fundamental task understanding  
            - **Best For:** Complex tasks requiring clear instructions  
            - **Speed:** Slower but more thorough optimization  

            ### üèÜ When to Use Each

            **Use BootstrapFewShot When:**  
            - You have good task instructions but need better examples  
            - Task performance depends heavily on few-shot examples  
            - You need fast optimization with good results  
            - Working with well-defined, straightforward tasks  

            **Use MIPRO When:**  
            - Task instructions are unclear or suboptimal  
            - Complex tasks requiring detailed guidance  
            - You want maximum performance regardless of time  
            - Working with novel or poorly-defined tasks  

            **Use Both Together When:**  
            - You want maximum performance improvement  
            - Working on production-critical applications  
            - Have sufficient time and computational resources  
            - Task complexity justifies multi-stage optimization  

            ### üöÄ Next Steps

            Ready to explore more optimization techniques? Try:  
            - **Custom Metrics** - Design domain-specific evaluation functions  
            - **Optimization Dashboard** - Build comprehensive tracking systems  

            Congratulations on mastering MIPRO optimization! üéâ
            """
        )
        if available_providers
        else ""
    )

    output.replace(cell7_out)
    return


if __name__ == "__main__":
    app.run()
